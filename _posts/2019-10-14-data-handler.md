---
layout: post
title: "ê°€ë°© crawling data handler"
author: "Eren"
tags: monde
---

MondeiqueëŠ” `ì—¬ì„± ê°€ë°©ì„ ìë™ìœ¼ë¡œ ì¸ì‹í•˜ê³  ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ê°€ë°© ê²€ìƒ‰ ì„œë¹„ìŠ¤` ë¼ëŠ” ì•„ì´ë””ì–´ë¡œ 2019 í•˜ë°˜ê¸° ì˜ˆë¹„ì°½ì—…íŒ¨í‚¤ì§€ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ëŸ° ì•„ì´ë””ì–´ì— ê¸°ë°˜ì€ AI ê°œë°œì´ì—ˆê³  ì´ë¥¼ ìœ„í•´ì„œëŠ” ë§ì€ ë°ì´í„°ë“¤ì´ í•„ìš”í–ˆê³  Amazon ì—ì„œ ì œê³µí•˜ëŠ” open dataset ë“¤ì„ í™œìš©í•˜ê³  ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ í†µí•´ í•™ìŠµì— ìš©ì´í•˜ê²Œ ë§Œë“¤ì–´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤.
<br>

ë³¸ í¬ìŠ¤íŠ¸ëŠ” ì´ëŸ¬í•œ ë°©ëŒ€í•œ data ë“¤ì„ Mondeique ì—ì„œëŠ” ì–´ë–»ê²Œ crawling ì„ í–ˆì—ˆê³  h5py file ì„ ë³€í™˜í–ˆëŠ”ì§€ ì„œìˆ í•˜ê³ ì í•©ë‹ˆë‹¤.
<br>
1. Amazon h5py to jpg converter
2. web-crawling
 
<hr>
<br>
 
## Amazon h5py to jpg converter

Amazonì—ì„œëŠ” open dataset ìœ¼ë¡œ ì´ëŸ° ë°©ëŒ€í•œ data ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. ê·¸ë ‡ì§€ë§Œ í•´ë‹¹ fileì€ h5py íŒŒì¼ í˜•íƒœì˜€ê³  ìš°ë¦¬ëŠ” ì´ë¥¼ jpgë¡œ ë³€í™˜í•´ì•¼ë§Œ í–ˆìŠµë‹ˆë‹¤. codeëŠ” ìƒê°ë³´ë‹¤ ì•„ì£¼ ê°„ë‹¨í–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ h5py ê°€ ê°€ì§€ê³  ìˆëŠ” keyë¥¼ íŒŒì•…í•œë‹¤ìŒ arrayë¡œë¶€í„° ì½ì–´ë“œë¦° ë’¤ local ì— ì €ì¥í•˜ê¸°ë§Œ í•˜ë©´ ëìŠµë‹ˆë‹¤.

{% highlight python %}
hdf = h5py.File('./data/bag_image/handbag_128.hdf5', 'r')
print(hdf.keys()) : <KeysViewHDF5 ['imgs']>

new_height = 227
new_width = 227

with h5py.File('./data/bag_image/handbag_128.hdf5', 'r') as f:
    data = f['imgs']
    for i in range(len(data)):
        im = Image.fromarray(data[i])
        new_im= im.resize((new_height, new_width), Image.ANTIALIAS)
#         im.save("./data/bag_image/hdf5_bag_128_{}.jpg".format(i)) # 128*128 ë¡œ ì €ì¥í•˜ëŠ” ê²½ìš°ì— ì‚¬ìš©
        new_im.save("./data/bag_image/hdf5_bag_227_{}.jpg".format(i))
{% endhighlight %}

## web-crawling 

web crawling ì˜ ê²½ìš° ì‡¼í•‘ëª°ë§ˆë‹¤ form ì´ ì¡°ê¸ˆì”©ì€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— í™ˆí˜ì´ì§€ ê°ê°ì„ ë¶„ì„í•´ `BeautifulSoup` ì„ ì´ìš©í•´ì„œ crawling ì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” luzzibag ì´ë¼ëŠ” ì‡¼í•‘ëª°ì„ ì˜ˆì‹œë¡œ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.

{% highlight python %}
## ìˆ„ë”ë°±

print("\n")
print("---------------------------------------------------------------------------")
print("ìˆ„ë”ë°± ì‚¬ì§„ url ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤")
print("---------------------------------------------------------------------------")
print("\n")

current_page = 1

img_url_list = []
shoulder_img_url_list = []
            
while current_page <= 2:
    print(str(current_page) + "ë²ˆì§¸ í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤!")
    url = 'http://www.luzzibag.com/category/%EC%88%84%EB%8D%94%EB%B0%B1/27/' + '?page=' + str(current_page)
    web = urlopen(url)
    source = BeautifulSoup(web, 'html.parser')
    for a in source.find_all('div', {"class" : "prdImg_thumb" }):
        for url in a.find_all("img"):
            if url["src"].startswith('//www.luzzibag.com/web/product/tiny'):
                img_url_list.append(url["src"])
                
    current_page = current_page + 1 
    time.sleep(5)

shoulder_img_url_list = img_url_list
                

## í† íŠ¸ë°±

print("\n")
print("---------------------------------------------------------------------------")
print("í† íŠ¸ë°± ì‚¬ì§„ url ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤")
print("---------------------------------------------------------------------------")
print("\n")

img_url_list = []
tote_img_url_list = []
            
print("1ë²ˆì§¸ í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤!")
url = 'http://www.luzzibag.com/category/%ED%86%A0%ED%8A%B8%EB%B0%B1/28/'
web = urlopen(url)
source = BeautifulSoup(web, 'html.parser')
for a in source.find_all('div', {"class" : "prdImg_thumb" }):
    for url in a.find_all("img"):
        if url["src"].startswith('//www.luzzibag.com/web/product/tiny'):
            img_url_list.append(url["src"])
            
tote_img_url_list = img_url_list 

{% endhighlight %}

ì´ë ‡ê²Œ ë§Œë“  listë¥¼ ê°„ë‹¨í•˜ê²Œ ì •ì œí•œ ë’¤ì— csv file ë¡œ ì €ì¥í•©ë‹ˆë‹¤. ë³´í†µ csv file ë¡œ ì €ì¥í•˜ëŠ” ì´ìœ ëŠ” ë‚˜ì¤‘ì— í•™ìŠµì—ì„œ DataLoader ê°€ í›¨ì”¬ ì‰½ê²Œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê²Œ í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.

{% highlight python %}
## ê²¹ì¹˜ëŠ” image ë°œê²¬í•´ì„œ ì‚­ì œí•˜ê¸° 

total_img_url_list = shoulder_img_url_list + tote_img_url_list 

len(total_img_url_list)

total_img_url_list = list(set(total_img_url_list))

len(total_img_url_list)

## Write to csv file 

import csv

with open('Luzzi_all_bag.csv', 'w', newline='') as f:
    writer = csv.writer(f, quoting=csv.QUOTE_ALL)
    writer.writerow(total_img_url_list)
    
{% endhighlight %}

ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” handbag dataset ì„ ì–´ë–»ê²Œ ê°€ì ¸ì˜¤ê³  ì •ì œí–ˆëŠ”ì§€ì— ëŒ€í•´ì„œ ì„œìˆ í–ˆìŠµë‹ˆë‹¤. ì‚¬ì‹¤ í¬ìŠ¤íŠ¸ë¡œë„ ë‚¨ê¸¸ í•„ìš”ê°€ ì—†ì„ ì •ë„ë¡œ ê°„ë‹¨í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ë…¼ë¦¬ë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, `web-crawling` ì´ë¼ëŠ” ë¶„ì•¼ì— ìƒì†Œí•¨ì„ ëŠê»´ì„œ ì–´ë ¤ì›€ì„ ê²ªëŠ” ì‚¬ëŒë“¤ì—ê²Œ ì‹¤ì œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ í›¨ì”¬ ì •ë³´ë¥¼ ìŠµë“í•˜ê¸°ì— ìš©ì´í•˜ë‹¤ê³  ìƒê°í–ˆìŠµë‹ˆë‹¤. ê°„ë‹¨í•œ ì•Œê³ ë¦¬ì¦˜ë„ ì°¨ê·¼ì°¨ê·¼ stackì„ ìŒ“ì•„ê°€ë©° ì ì–´ê°€ëŠ” ìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•˜ê¸°ì— ì´ ê¸€ë„ ì–¸ì  ê°„ ê°€ì¹˜ê°€ ìˆëŠ” ê¸€ì´ ë  ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤ ğŸ‘ 







